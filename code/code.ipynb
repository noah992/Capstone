{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 6 Capstone: Improve search performance using image recognition\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "\n",
    "Iamge recognition is widely used and it improves our search performance a lot.\n",
    "One major use case of image recogition is deployment on EC site. Some EC sites such as Amazon, ZARA, has a search option where you can upload a picture of items that you want to buy and then the website will look for the items or similar items for you.\n",
    "\n",
    "Especially it is deployed on many fashion related EC sites because it is common to happen that you are not sure what brand the good looking jacket is and it is difficult to describe how it looks.\n",
    "You could not find the jacket describing that the jacket is green, made with silk or else because all of these features can be applied on many various kinds of cloths. It is very helpful if you can use a picture of the jacket and the websites find it for you.\n",
    "\n",
    "I worked on to create similar function with sightseeing objects using a multi-classification model trained with images of several sightseeing objects.\n",
    "This model recognizes which object is in the picture. Convolutional neural network model is appropriate for this problem as I use images as prediction values.\n",
    "\n",
    "One initial technical problem is that I could not deal with very many kinds of sightseeing objects due to machine spec and time constraint so that I chosed only 10 objects which are somewhat related. After all I decided to use 10 kinds of cathedrals as these are famous places for visitors and I bet their appearances are relatively similar to one another so that many visitors could not be sure which cathedral they want to go.\n",
    "\n",
    "**Summary**\n",
    "\n",
    "* Purpose: Model which recognizes cathedrals with its image\n",
    "* Method: Multi-classification model with convolutional neural network.\n",
    "* Metric: Accuracy\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "1. Data collecting\n",
    "    1. Gather 10 kinds of cathedrals\n",
    "2. EDA\n",
    "    1. Filter images which are not `RGB`\n",
    "    2. Resize to `256 x 256`\n",
    "3. Modeling\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data collecting\n",
    "\n",
    "\n",
    "I will gather images of 10 cathedrals from Google search result using [Image Downloader](https://chrome.google.com/webstore/detail/image-downloader/cnpniohnfphhjihaiiggeabnkjhpaldj)\n",
    "\n",
    "Some images has a front side of an object, some has back side of the object and some others are taken at very far away. If I take all of these images into my model together, it would not learn effectively so that I need to gather images which are visually similar.\n",
    "\n",
    "Below 2 images contain same object, but it looks different as they were taken at different angle and distance. Also second one is colored by light.\n",
    "Data set of these combination would not be rational so that I gather images similar to first one.\n",
    "\n",
    "<img src=\"https://github.com/noah992/Capstone/blob/main/image/data-collecting-01.JPG?raw=true\" width=\"200pt\"> <img src=\"https://github.com/noah992/Capstone/blob/main/image/data-collecting-02.JPG?raw=true\" width=\"300pt\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are the cathedrals that I will train a model with.\n",
    "\n",
    "|Cathedral|\n",
    "|-|\n",
    "|The Cathedral Church of St. John the Divine|\n",
    "|Catedral Metropolitana Nossa Senhora Aparecida|\n",
    "|Patrick’s Cathedral|\n",
    "|Catedral de La Plato|\n",
    "|Cathedral Basilica of the Sacred Heart|\n",
    "|Cathédrale Notre-Dame de Chartres|\n",
    "|Cattedrale di Santa Maria del Fiore|\n",
    "|Kölner Dom|\n",
    "|St. Paul's Cathedral|\n",
    "|Patriarchal Cathedral ST. Alexander Nevsky|\n",
    "\n",
    "I collected around 50 images for each. This is quite few so I duplicated the images and duplication resulted in better accuracy score so that I will keep using duplicated images\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import images of the objects\n",
    "\n",
    "I found some images are gray scale, and some other has alpha information. I want to only have RGB images because if array shape of images are different, I cannot feed them into a model.\n",
    "\n",
    "Third index of the data shape stores color information and this number must be same\n",
    "\n",
    "|Color|Shape|\n",
    "|-|-|\n",
    "|RGBα|(1200, 450,  `4`)|\n",
    "|RGB|(428, 678, `3`)|\n",
    "|Gray|(564, 680, `1`)|\n",
    "\n",
    "\n",
    "I will filter out images which are not `RGB` , and also convert them to `numpy` data to check the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get image and return list of numpy image data\n",
    "def get_images(path):\n",
    "    \n",
    "    images = [] # Store binary image data\n",
    "    mode = [] # Store mode of images. I will get only 'RGB' images\n",
    "    \n",
    "    for image in os.listdir(path):\n",
    "        img = Image.open(path+image)\n",
    "        if img.mode=='RGB':\n",
    "            images.append(np.array(img))\n",
    "            mode.append(img.mode)\n",
    "                        \n",
    "    # Check color scale of images. If there are gray scale (L) images, the data shape will be different so I filtered gray scale images\n",
    "    print('Mode', pd.Series(mode).value_counts()) \n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store images in variables\n",
    "Below cell returns numbner of `RGB` images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '../data/PatricksCathedral/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-73009f37f94e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpatric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../data/PatricksCathedral/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msenhora\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../data/Catedral Metropolitana Nossa Senhora Aparecida/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../data/Catedral de La Plata/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mbasilica\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../data/Cathedral Basilica of the Sacred Heart/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mchartres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../data/Cathédrale Notre-Dame de Chartres/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-377c5a82cd18>\u001b[0m in \u001b[0;36mget_images\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# Store mode of images. I will get only 'RGB' images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'RGB'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '../data/PatricksCathedral/'"
     ]
    }
   ],
   "source": [
    "patric = get_images('../image/PatricksCathedral/')\n",
    "senhora = get_images('../image/Catedral Metropolitana Nossa Senhora Aparecida/')\n",
    "plata = get_images('../image/Catedral de La Plata/')\n",
    "basilica = get_images('../image/Cathedral Basilica of the Sacred Heart/')\n",
    "chartres = get_images('../image/Cathédrale Notre-Dame de Chartres/')\n",
    "santa = get_images('../image/Cattedrale di Santa Maria del Fiore/')\n",
    "kolner = get_images('../image/Kölner Dom/')\n",
    "paul = get_images('../image/St. Paul\\'s Cathedral/')\n",
    "john = get_images('../image/The Cathedral Church of St. John the Divine/')\n",
    "sbeti = get_images('../image/Patriarchal Cathedral ST. Alexander Nevsky/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample of image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(plata[0])\n",
    "plt.title('Catedral de La Plata')\n",
    "plt.xlabel('Width')\n",
    "plt.ylabel('Height');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(basilica[0])\n",
    "plt.title('Cathedral Basilica of the Sacred Heart')\n",
    "plt.xlabel('Width')\n",
    "plt.ylabel('Height');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape\n",
    "Now I will check `height` and `width` of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plata[0].shape)\n",
    "print(plata[19].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All images must have same shape so I will resize images to get consistent `height` and `width`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to trim images. This convert images to square of 256 x 256\n",
    "# Also this function convert numpy data to image data\n",
    "\n",
    "def trimming(imgs, n):\n",
    "    result = Image.fromarray(imgs[n])\n",
    "    return result.resize((256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I apply above method on all images\n",
    "And store all images to a list, `trim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store objects. This keys will be target values equivalent to the object\n",
    "objects = {\n",
    "    0: plata,\n",
    "    1: basilica, \n",
    "    2: senhora,\n",
    "    3: john,   \n",
    "    4: sbeti, \n",
    "    5: patric,\n",
    "    6: chartres,\n",
    "    7: santa,\n",
    "    8: kolner,\n",
    "    9: paul,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim all images and combine them into single list\n",
    "trim = []\n",
    "for key in objects.keys():\n",
    "    for obj in range(len(objects[key])):\n",
    "        trim.append(trimming(objects[key], obj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before resize..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(plata[0])\n",
    "plt.title('Before resized image (225, 225, 3)')\n",
    "plt.xlabel('pixel')\n",
    "plt.ylabel('pixel')\n",
    "plata[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(trim[0])\n",
    "plt.title('After resized image (256, 256, 3)')\n",
    "plt.xlabel('pixel')\n",
    "plt.ylabel('pixel')\n",
    "np.array(trim[0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set target variable\n",
    "`key` of dictionary `objects` is assigned to target value equivalent to the object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = []\n",
    "for value in objects.keys():\n",
    "    for _ in range(len(objects[value])):\n",
    "        target.append(value)\n",
    "\n",
    "print('Count of target values')\n",
    "pd.Series(target).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert images to numpy data and store in pandas Series\n",
    "features = pd.Series([np.array(img) for img in trim])\n",
    "target = pd.Series(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import utils\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale\n",
    "features /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features.shape)\n",
    "print(features[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `features` has `1312` rows and each row shapes `(256, 256, 3)`. I want to convert this to `(1312, 256, 256, 3)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all images to 1 dimension\n",
    "features = [i.reshape(196608) for i in features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up X and y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(features).reshape(1312, 256, 256, 3)\n",
    "y = utils.to_categorical(target)\n",
    "\n",
    "print(X.shape)\n",
    "print(X[12].shape)\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "I will use convolutional neural network model with `Keras Sequential` and `accuracy` for metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After several trials with other parameters, this model works good\n",
    "cnn = Sequential()\n",
    "cnn.add(Conv2D(filters=16, kernel_size=(5,5), activation='relu', input_shape=(256, 256, 3)))\n",
    "cnn.add(MaxPooling2D(pool_size=(10,10))) \n",
    "cnn.add(Conv2D(256, kernel_size=(5,5),  activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(10,10)))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(128, activation='relu'))\n",
    "cnn.add(Dense(64, activation='relu'))\n",
    "cnn.add(Dense(10, activation='softmax'))\n",
    "cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = cnn.fit(X_train, y_train, batch_size=128, validation_data=(X_test, y_test), epochs=40, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "`90%` accuracy. Surely it can be closer to 100% with more images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reccomendation\n",
    "\n",
    "I made an app to demonstrate how you could use the image recognition on your website.\n",
    "\n",
    "[Visit to demonstration](https://github.com/noah992/Capstone/blob/main/image/demonstration.gif)\n",
    "\n",
    "In this app, I fed an url of image of a cathedral which is located in Brazil.\n",
    "\n",
    "\n",
    "You can see what image I fed on the second screen.\n",
    "This app loaded the image and neural network model which I assembled worked on the backend and generated prediction of what the object was in the image.\n",
    "Based on the result of the prediction, it navigated you to the location with google map.\n",
    "\n",
    "You may consider to deploy image recognition as search option as it definitely improve user experience.\n",
    "As I mentioned in `Problem`, visitors could not be sure which cathedral is which as they looks similar so it could be hard to identify a cathedral for them by searching with some keywords however, they would be happy using image search because it would be much easier and faster\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step\n",
    "\n",
    "I used images which are taken at similar angle, similar distance and similar color. So if I feed images which were taken in different angle or with different light color, this model is likely to fail to predict correct location.\n",
    "\n",
    "I could improve the accuracy by feeding various kinds of pictures in terms of angle and color, this app will be greater as it can be more flexible on picture variation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
